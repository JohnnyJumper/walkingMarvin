{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "env = gym.make('Marvin-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to gather the data from game samples\n",
    "\n",
    "\n",
    "num_episodes = 10000\n",
    "num_steps_per_episode = 1000\n",
    "\n",
    "\n",
    "traning_data = []\n",
    "for _ in range(num_episodes):\n",
    "    prev_observation = env.reset()\n",
    "    \n",
    "    for step in range(num_steps_per_episode):        \n",
    "        # Let's choose a random action\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Make your step and collect results\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        traning_data.append([prev_observation, action, reward, done])\n",
    "        prev_observation = observation\n",
    "        if done == True:\n",
    "            break \n",
    "    \n",
    "\n",
    "data = pd.DataFrame(traning_data, columns=['Observation', 'Action', 'Reward', 'Done'])\n",
    "# data.to_csv('./walking-marvin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piece of code to read the data from csv file\n",
    "data = pd.read_csv('./walking-marvin.csv')\n",
    "data.columns = ['Index', 'Observation', 'Action', 'Reward', 'Done']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412394\n",
      "412394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11890ac88>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try to explore the data we gath\n",
    "\n",
    "\n",
    "data.plot.scatter(x='Index', y='Reward')\n",
    "print(len(data[data['Reward'] > 0]))\n",
    "print(len(data[data['Reward'] < 0].head(412394)))\n",
    "\n",
    "positiveRewards = data[data['Reward'] > 0]\n",
    "negativeRewards = data[data['Reward'] != -100].head(412394)\n",
    "\n",
    "positiveRewards.plot(y='Reward', figsize=(18, 16))\n",
    "negativeRewards.plot(y='Reward', figsize=(18, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I want to create the neuronNetwork that will take an observation of a enviroment (24 dimensional vector) and returns\n",
    " me an action (4 dimensional vector) I then send this action to env.step and receive a reward. Somehow i need to update\n",
    " the weights of my network using the reward. My reward should be as big as possible. (1 - reward) is my error? \n",
    " Looks like so. \n",
    " I need three layers of neurons. each neuron has a weight property, activation function, function to update the weight\n",
    " Each layer consist of several neurons. I need to use the weight inside the neuron. so layers just run neurons commands\n",
    " and play as a coordinator.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_and_sum(a, b):\n",
    "    result = 0\n",
    "    for i in range(len(a)):\n",
    "        result += a[i] * b[i]\n",
    "    return result\n",
    "\n",
    "\n",
    "class Neuron(object):\n",
    "    def __init__(self, input_counts = 1, activation_function = math.tanh,\n",
    "                update_function=None, learning_rate = 0.001, input_layer = False, value = None):\n",
    "        self.input_layer = input_layer\n",
    "        if self.input_layer == False:\n",
    "            self.weights = [random.uniform(-1, 1)] * input_counts\n",
    "        self.value = value\n",
    "        self.activation_function = activation_function\n",
    "        self.update_function = self.update_weights if update_function == None else update_function\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def setValue(self, value):\n",
    "        self.value = value\n",
    "        \n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n--------------\\nself.weights: {}\\nself.value: {}\\n--------------\\n\".format(self.weights, self.value)\n",
    "    \n",
    "        \n",
    "    def activate(self, inputs = None):\n",
    "        if (self.input_layer):\n",
    "            return self.value\n",
    "        self.setValue(self.activation_function(mult_and_sum(inputs, self.weights)))\n",
    "        return self.value\n",
    "    \n",
    "    def update_weights(self, inp, update):\n",
    "        # I should update the weight only if the reward is bigger? \n",
    "        errors = self.learning_rate * (update - self.activate(inp))\n",
    "        self.weights = np.add(self.weights, np.multiply(inp, errors))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_classifiyer(x):\n",
    "    return x\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, neuron_count, input_counts = 1, activation_function = linear_classifiyer):\n",
    "        random.seed()\n",
    "        self.neurons = []\n",
    "        self.input_layer = True if input_counts == 1 else False\n",
    "       \n",
    "        for _ in range(neuron_count):\n",
    "            self.neurons.append(Neuron(input_counts, input_layer = self.input_layer))\n",
    "    \n",
    "    def setValues(self, inputs):\n",
    "        for index, neuron in enumerate(self.neurons):\n",
    "            neuron.setValue(inputs[index])\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        message = \"[input_layer: {},\\nneurons: {}]\"\n",
    "        return message.format(self.input_layer, len(self.neurons))\n",
    "\n",
    "    def update(self, inputs, reward):\n",
    "        if (self.input_layer):\n",
    "            return \n",
    "        for neuron in self.neurons:\n",
    "            neuron.update_weights(inputs, reward)\n",
    "    \n",
    "    def output(self, inputs = None):\n",
    "        layer_output = []\n",
    "        for neuron in self.neurons:\n",
    "            layer_output.append(neuron.activate(inputs))\n",
    "        return layer_output\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain(object):\n",
    "    def __init__(self, input_counts, output_counts, hidden_layers, neurons_per_hidden_layer):\n",
    "        self.layers = []\n",
    "        self.input_layer = Layer(neuron_count = input_counts, input_counts = 1)\n",
    "        self.layers.append(self.input_layer)\n",
    "        transition_input_counts = input_counts\n",
    "        for _ in range(hidden_layers):\n",
    "            self.layers.append( Layer(neurons_per_hidden_layer, input_counts=transition_input_counts,\n",
    "                                        activation_function = math.tanh))\n",
    "            transition_input_counts = neurons_per_hidden_layer\n",
    "            \n",
    "        self.output_layer = Layer(output_counts, input_counts = transition_input_counts, activation_function=math.tanh)\n",
    "        self.layers.append(self.output_layer)\n",
    "        \n",
    "    def __str__(self):\n",
    "        message = \"\\n\\nlayers: {}\\ninput_counts: {},\\noutput_counts: {},\\nhidden_layers: {}\\n\\n\"\n",
    "        return message.format(len(self.layers),\n",
    "                              len(self.layers[0].neurons),\n",
    "                              len(self.layers[-1].neurons),\n",
    "                              len(self.layers) - 2)\n",
    "    \n",
    "    def generate_action(self, observation):\n",
    "        self.input_layer.setValues(observation)\n",
    "        inputs  = self.input_layer.output()\n",
    "        for hidden_layer in self.layers[1: -1]:\n",
    "            inputs = hidden_layer.output(inputs)\n",
    "        return self.output_layer.output(inputs)\n",
    "    \n",
    "    def learn(self, observation, reward):\n",
    "        self.input_layer.setValues(observation)\n",
    "        layer_input = self.input_layer.output()\n",
    "        for layer in self.layers[1:]:\n",
    "            layer.update(layer_input, reward)\n",
    "            layer_input = layer.output(layer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "marvin = Brain(24, 4, 1, 32)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "\n",
      "100 \n",
      "\n",
      "200 \n",
      "\n",
      "300 \n",
      "\n",
      "400 \n",
      "\n",
      "500 \n",
      "\n",
      "600 \n",
      "\n",
      "700 \n",
      "\n",
      "800 \n",
      "\n",
      "900 \n",
      "\n",
      "1000 \n",
      "\n",
      "1100 \n",
      "\n",
      "1200 \n",
      "\n",
      "1300 \n",
      "\n",
      "1400 \n",
      "\n",
      "1500 \n",
      "\n",
      "1600 \n",
      "\n",
      "1700 \n",
      "\n",
      "1800 \n",
      "\n",
      "1900 \n",
      "\n",
      "2000 \n",
      "\n",
      "2100 \n",
      "\n",
      "2200 \n",
      "\n",
      "2300 \n",
      "\n",
      "2400 \n",
      "\n",
      "2500 \n",
      "\n",
      "2600 \n",
      "\n",
      "2700 \n",
      "\n",
      "2800 \n",
      "\n",
      "2900 \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 3000\n",
    "max_steps_per_episode = 2000\n",
    "\n",
    "learning_rate = 0.001\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "rewards_all_episodes = []\n",
    "\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    if (episode % 100 == 0):\n",
    "        print(\"{} \\n\".format(episode))\n",
    "    for step in range(max_steps_per_episode):\n",
    "         # Exploration-Explotation trade off\n",
    "#         env.render()\n",
    "        exploration_rate_treshold = random.uniform(0,1)\n",
    "        if exploration_rate_treshold >= exploration_rate:\n",
    "            action = marvin.generate_action(observation)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "#         print(reward, done, info)\n",
    "        # Update my weights according to reward on each neuron in my brain.\n",
    "        marvin.learn(observation, reward)\n",
    "        rewards_current_episode += reward\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "#     #Exploration decay logic \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode) \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Calculate and print the average reward per 1000 episodes\n",
    "perE = 1000\n",
    "\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes / perE)\n",
    "count = perE\n",
    "print(\"*** Average reward per {} episodes ***\\n\".format(perE))\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/ 1000)))\n",
    "    count += perE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'DataFramerame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-28e48d54177e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpanda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFramerame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards_all_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpanda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./result.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'DataFramerame'"
     ]
    }
   ],
   "source": [
    "panda = pd.DataFramerame(rewards_all_episodes)\n",
    "panda.to_csv('./result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "for step in range(max_steps_per_episode):\n",
    "    env.render()\n",
    "    action = marvin.generate_action(observation)\n",
    "    new_observation, reward, done, info = env.step(action)\n",
    "    marvin.learn(observation, reward)\n",
    "    if (done == True):\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
